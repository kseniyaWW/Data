#Set up and connect Twitter

Twitter connectAPI key
```{r}
#install.packages("twitteR")

#using the twitteR package ####
library(twitteR)

#From the twitter app (Keys and tokens)
api_key <- "insert" # API key 
api_secret <- "insert" #API secret key BEARER TOKEN
token <- "insert" #token 
token_secret <- "insert" #token secret


setup_twitter_oauth(api_key, api_secret, token, token_secret) # setup for accessing twitter using the information above

tweets <- searchTwitter(q="rstats")

tweets <- searchTwitter('#fakenews AND [facebook OR #facebook]', n=1000, lang = "en") # the function searchTwitter search for tweets based on the specified parameters

tweets.df <-twListToDF(tweets) # creates a data frame with one row per tweet

tweetDF <- as.data.frame(tweets.df)

View(tweetDF)

```


#Timeline scrap
```{r}
library(rtweet) 
library(tidytext)
library(ggpubr) 
library(tidyverse) 
library(httpuv)
library(ROAuth)
library(base64enc)
library(twitteR)

cred <- OAuthFactory$new(consumerKey = "insert_consumer_key",
                     consumerSecret = "insert_consumer_key_secret",
                     requestURL = "https:////api.twitter.com/oauth/request_token",
                     accessURL ="https://api.twitter.com/oauth/access_token",
                     authURL = "https://api.twitter.com/oauth/authorize")

save(cred,file="twitter authentication.Rdata")
load("twitter authentication.Rdata")

setup_twitter_oauth("insert_consumer_key", "insert_consumer_key_secret",
                    "access_token", 
                    "access_token_secret")
1

Nike10.03 <- userTimeline('NikeService', n=1000)
Nike10.03 <- twListToDF(Nike10.03)
View(Nike10.03)

#that piece of code converts directly most recent tweets (max 3200) posted to the DF
nike <- get_timeline("NikeService", n=3200)
View(nike)




```

#Web-scrapping
```{r}
# read the library (rvest)
library(rvest)

#get all links: daily basis
p0001 <- read_html(unlist("https://www.borgerforslag.dk/"))

links <- p0001 %>% 
  html_nodes("a") %>% 
  html_attr("href")

links <- data.frame(links)

head(links)

links <- as.character(unique(links[grep("/se-og-stoet-forslag/", links$links),]))

links <- paste("https://www.borgerforslag.dk",links,sep="")

date <- rep(Sys.time(),length(links))
dailylinks <- data.frame(links,date)
colnames(dailylinks) = c("link","linkdate")

head(dailylinks)

#scraping all urls from the crawl ####
proposal <- c()
titles <- c()

for(url in links){
  page <- read_html(unlist(url))
  
  texts <- page %>% 
    html_node("div.article") %>%
    html_text() 
  
  titel <- page %>%
    html_nodes("div.cc552X") %>%
    html_text()
  
  titles <- append(titles, titel[1])
  proposal <- append(proposal, texts)
}

proposalData <- data.frame(titles,proposal)

View(proposalData)
```
```{r}
library(devtools)
library(Rfacebook)
library(RCurl)
#install_github("Rfacebook", "pablobarbera", subdir="Rfacebook")
fb_oauth <- fbOAuth(app_id="2896318003969298", app_secret="dca4e90a7761abbdfdc7498f887a9201",extended_permissions = TRUE)

```







